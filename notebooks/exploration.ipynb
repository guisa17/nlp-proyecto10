{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee84fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guisa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cc2229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de datos cargados:\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "# Empezamos con 5000 datos\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:5000]\")\n",
    "\n",
    "# Ejemplo\n",
    "print(\"Ejemplo de datos cargados:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13985929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Etiquetas de los datos: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas de los datos\n",
    "labels = dataset.features[\"label\"].names\n",
    "print(\"\\nEtiquetas de los datos:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d6c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conteo de cada etiqueta: Counter({'Sci/Tech': 1497, 'Business': 1236, 'World': 1235, 'Sports': 1032})\n"
     ]
    }
   ],
   "source": [
    "# Conteo de cada etiqueta\n",
    "from collections import Counter\n",
    "counts = Counter([labels[x[\"label\"]] for x in dataset])\n",
    "print(\"\\nConteo de cada etiqueta:\", counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed533f",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9eec1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7964c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94246020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "200276f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de texto preprocesado:\n",
      "['wall', 'st', 'bears', 'claw', 'back', 'black', 'reuters', 'reuters', 'shortsellers', 'wall', 'streets', 'dwindlingband', 'ultracynics', 'seeing', 'green']\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamos los datos\n",
    "text = [preprocess(example[\"text\"]) for example in dataset]\n",
    "\n",
    "print(\"\\nEjemplo de texto preprocesado:\")\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b09af0",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7b995",
   "metadata": {},
   "source": [
    "[ ] Code provisional TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63ef78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f40c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensiones de la matriz TF-IDF: (5000, 3000)\n",
      "Primeros 10 terminos del vocabulario:\n",
      "['wall', 'st', 'back', 'black', 'reuters', 'streets', 'seeing', 'green', 'looks', 'toward']\n"
     ]
    }
   ],
   "source": [
    "# Convertimos los token preprocesados a texto limpio\n",
    "clean_text = [\" \".join(t) for t in text]\n",
    "\n",
    "# Vectorizacion TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X_tfidf = vectorizer.fit_transform(clean_text)\n",
    "\n",
    "print(\"\\nDimensiones de la matriz TF-IDF:\", X_tfidf.shape)\n",
    "print(\"Primeros 10 terminos del vocabulario:\")\n",
    "print(list(vectorizer.vocabulary_.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e91691",
   "metadata": {},
   "source": [
    "### Visualizción términos más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c34928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           term  avg_tfidf\n",
      "2207    reuters   0.023289\n",
      "128          ap   0.022766\n",
      "2815         us   0.020208\n",
      "1739        new   0.019059\n",
      "2271       said   0.015827\n",
      "1085     google   0.012393\n",
      "177      athens   0.011755\n",
      "2767    tuesday   0.011667\n",
      "1786        oil   0.011651\n",
      "969       first   0.010992\n",
      "2907  wednesday   0.010756\n",
      "1256        inc   0.009802\n",
      "530     company   0.009507\n",
      "2004     prices   0.009407\n",
      "2782        two   0.009384\n",
      "2698   thursday   0.009139\n",
      "1790    olympic   0.009083\n",
      "2992       york   0.008184\n",
      "1078       gold   0.008090\n",
      "2964      world   0.008046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calcular TF-IDF promedio por término\n",
    "mean_tfidf = np.asarray(X_tfidf.mean(axis=0)).flatten()\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Crear tabla y ordenar\n",
    "df_tfidf = pd.DataFrame({\"term\": terms, \"avg_tfidf\": mean_tfidf})\n",
    "top_terms = df_tfidf.sort_values(by=\"avg_tfidf\", ascending=False).head(20)\n",
    "print(top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d305f",
   "metadata": {},
   "source": [
    "### Matriz de coocurrencias y PMI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f1337aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6cd563",
   "metadata": {},
   "source": [
    "- Creamos la matriz de coocurrencisa para hallar aquellas palabras que aparezcan juntas en un mismo contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a72540",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4\n",
    "cooccurrence_counts = defaultdict(Counter)\n",
    "\n",
    "for tokens in text:\n",
    "    for i, token in enumerate(tokens):\n",
    "        window = tokens[max(i - window_size, 0): i] + tokens[i+1: i+1+window_size]\n",
    "        for neighbor in window:\n",
    "            cooccurrence_counts[token][neighbor] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e7045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabras relacionadas con 'president':\n",
      "  gleam: 9.57\n",
      "  unitedstatesi: 9.57\n",
      "  unelectable: 9.57\n",
      "  corporationand: 9.57\n",
      "  nominees: 9.57\n",
      "  anexecutive: 9.57\n",
      "  ingrained: 9.57\n",
      "  distrustof: 9.57\n",
      "  concentrated: 9.57\n",
      "  referendumthat: 9.57\n",
      "\n",
      "Palabras relacionadas con 'game':\n",
      "  ballpark: 11.20\n",
      "  whotold: 10.20\n",
      "  microgames: 10.20\n",
      "  catwoman: 10.20\n",
      "  mosquitos: 10.20\n",
      "  gamecopying: 10.20\n",
      "  fiftyfive: 10.20\n",
      "  dislocating: 10.20\n",
      "  forthcoming: 10.20\n",
      "  convergence: 10.20\n",
      "\n",
      "Palabras relacionadas con 'company':\n",
      "  exemployees: 9.50\n",
      "  marlboroughbased: 9.50\n",
      "  ecm: 9.50\n",
      "  anoil: 9.50\n",
      "  picnic: 9.50\n",
      "  sidestepped: 8.50\n",
      "  shuts: 8.50\n",
      "  clockwork: 8.50\n",
      "  wrests: 8.50\n",
      "  defunct: 8.50\n",
      "\n",
      "Palabras relacionadas con 'computer':\n",
      "  unfortunately: 11.19\n",
      "  makerdell: 10.19\n",
      "  locks: 10.19\n",
      "  toggle: 10.19\n",
      "  problemsolving: 10.19\n",
      "  teambuilding: 10.19\n",
      "  treats: 10.19\n",
      "  contamination: 10.19\n",
      "  resellers: 10.19\n",
      "  interrupted: 10.19\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Conteos totales\n",
    "word_counts = Counter(itertools.chain(*text))\n",
    "total_words = sum(word_counts.values())\n",
    "\n",
    "def compute_pmi(word, context):\n",
    "    pw = word_counts[word] / total_words\n",
    "    pc = word_counts[context] / total_words\n",
    "    pwc = cooccurrence_counts[word][context] / total_words\n",
    "    if pwc == 0:\n",
    "        return 0\n",
    "    return math.log2(pwc / (pw * pc))\n",
    "\n",
    "# Tabla PMI ejemplo\n",
    "sample_words = [\"president\", \"game\", \"company\", \"computer\"]\n",
    "for word in sample_words:\n",
    "    print(f\"\\nPalabras relacionadas con '{word}':\")\n",
    "    related = [(w, compute_pmi(word, w)) for w in cooccurrence_counts[word]]\n",
    "    related = sorted(related, key=lambda x: -x[1])[:10]\n",
    "    for neighbor, score in related:\n",
    "        print(f\"  {neighbor}: {score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
