{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee84fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guisa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cc2229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guisa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\guisa\\.cache\\huggingface\\hub\\datasets--ag_news. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 120000/120000 [00:00<00:00, 656223.00 examples/s]\n",
      "Generating test split: 100%|██████████| 7600/7600 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de datos cargados:\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Empezamos con 5000 datos\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:5000]\")\n",
    "\n",
    "# Ejemplo\n",
    "print(\"Ejemplo de datos cargados:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13985929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Etiquetas de los datos: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# Estadísticas de los datos\n",
    "labels = dataset.features[\"label\"].names\n",
    "print(\"\\nEtiquetas de los datos:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d6c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conteo de cada etiqueta: Counter({'Sci/Tech': 1497, 'Business': 1236, 'World': 1235, 'Sports': 1032})\n"
     ]
    }
   ],
   "source": [
    "# Conteo de cada etiqueta\n",
    "from collections import Counter\n",
    "counts = Counter([labels[x[\"label\"]] for x in dataset])\n",
    "print(\"\\nConteo de cada etiqueta:\", counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed533f",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9eec1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7964c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guisa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94246020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "200276f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de texto preprocesado:\n",
      "['wall', 'st', 'bears', 'claw', 'back', 'black', 'reuters', 'reuters', 'shortsellers', 'wall', 'streets', 'dwindlingband', 'ultracynics', 'seeing', 'green']\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamos los datos\n",
    "text = [preprocess(example[\"text\"]) for example in dataset]\n",
    "\n",
    "print(\"\\nEjemplo de texto preprocesado:\")\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b09af0",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7b995",
   "metadata": {},
   "source": [
    "[ ] Code provisional TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e63ef78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97f40c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensiones de la matriz TF-IDF: (5000, 3000)\n",
      "Primeros 10 terminos del vocabulario:\n",
      "['wall', 'st', 'back', 'black', 'reuters', 'streets', 'seeing', 'green', 'looks', 'toward']\n"
     ]
    }
   ],
   "source": [
    "# Convertimos los token preprocesados a texto limpio\n",
    "clean_text = [\" \".join(t) for t in text]\n",
    "\n",
    "# Vectorizacion TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X_tfidf = vectorizer.fit_transform(clean_text)\n",
    "\n",
    "print(\"\\nDimensiones de la matriz TF-IDF:\", X_tfidf.shape)\n",
    "print(\"Primeros 10 terminos del vocabulario:\")\n",
    "print(list(vectorizer.vocabulary_.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e91691",
   "metadata": {},
   "source": [
    "### Visualizción términos más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c34928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           term  avg_tfidf\n",
      "2207    reuters   0.023289\n",
      "128          ap   0.022766\n",
      "2815         us   0.020208\n",
      "1739        new   0.019059\n",
      "2271       said   0.015827\n",
      "1085     google   0.012393\n",
      "177      athens   0.011755\n",
      "2767    tuesday   0.011667\n",
      "1786        oil   0.011651\n",
      "969       first   0.010992\n",
      "2907  wednesday   0.010756\n",
      "1256        inc   0.009802\n",
      "530     company   0.009507\n",
      "2004     prices   0.009407\n",
      "2782        two   0.009384\n",
      "2698   thursday   0.009139\n",
      "1790    olympic   0.009083\n",
      "2992       york   0.008184\n",
      "1078       gold   0.008090\n",
      "2964      world   0.008046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calcular TF-IDF promedio por término\n",
    "mean_tfidf = np.asarray(X_tfidf.mean(axis=0)).flatten()\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Crear tabla y ordenar\n",
    "df_tfidf = pd.DataFrame({\"term\": terms, \"avg_tfidf\": mean_tfidf})\n",
    "top_terms = df_tfidf.sort_values(by=\"avg_tfidf\", ascending=False).head(20)\n",
    "print(top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211c649",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
